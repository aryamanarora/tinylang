model:
  class: "Llama"
  config:
    n_positions: 1024
    n_embd: 64 # n_inner is 4x by default
    n_layer: 4
    n_head: 4
language:
  class: "PCFG"
  config:
    num_terminals: 20
    num_nonterminals: 10
    max_rhs_len: 10
    max_rules_per_nt: 5
    max_depth: 10
    head_position: "left"
    mask_nonquery: True
    no_sibling_queries: True
    no_child_queries: True
evaluators:
  - class: "SummaryEvaluator"
    config:
      run_every_n_steps: 1000
  - class: "AttentionEvaluator"
    config:
      run_every_n_steps: 5000
  - class: "ProbeEvaluator"
    config:
      run_every_n_steps: 5000
  - class: "InterchangeEvaluator"
    config:
      run_every_n_steps: 5000
training:
  train_batch_size: 16
  num_train_steps: 100000
  eval_batch_size: 16
  num_eval_steps: 100
  lr: 1e-4