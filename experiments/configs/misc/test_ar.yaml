model:
  class: "GPT2"
  config:
    n_positions: 1024
    n_embd: 128 # n_inner is 4x by default
    n_layer: 4
    n_head: 4
    # rope_theta: 1000.0
language:
  class: "AR"
  config:
    num_kv: 20
    max_length: 20
    min_length: 20
    query_type: "key"
    mask_nonquery: False
evaluators:
  - class: "SummaryEvaluator"
    config:
      run_every_n_steps: 200
  # - class: "AttentionEvaluator"
  #   config:
  #     run_every_n_steps: 200
  - class: "ProbeEvaluator"
    config:
      run_every_n_steps: 200
      do_dim_reduction: False
      do_sim: True
  # - class: "InterchangeEvaluator"
  #   config:
  #     run_every_n_steps: 200
training:
  train_batch_size: 16
  num_train_steps: 5000
  eval_batch_size: 64
  num_eval_steps: 2
  lr: 1e-4